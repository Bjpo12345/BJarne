{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Watson Analytics\n",
    "\n",
    "Watson Analytics is a Cloud Service offering AI capabilities via REST APIs. https://www.ibm.com/analytics/watson-analytics/us-en/index.html\n",
    "\n",
    "\n",
    "https://github.com/watson-developer-cloud/python-sdk\n",
    "\n",
    "```bash\n",
    "pip install watson-developer-cloud\n",
    "```\n",
    "The API documentation is here:\n",
    "https://www.ibm.com/watson/developercloud/visual-recognition/api/v3/curl.html?curl\n",
    "\n",
    "\n",
    "## Getting the Service Credentials\n",
    "\n",
    "Service credentials are required to access the APIs.\n",
    "\n",
    "To run locally or outside of Bluemix you need the `username` and `password` credentials for each service. (Service credentials are different from your Bluemix account email and password.)\n",
    "\n",
    "To create an instance of the service:\n",
    "\n",
    "  * Log in to [Bluemix](https://console.ng.bluemix.net).\n",
    "  * Create an instance of the service:\n",
    "    1. In the Bluemix Catalog, select the Watson service you want to use. For our example, select under *Watson* the *Visual Recognition* service.\n",
    "    2. Type a unique name for the service instance in the Service name field. For example, type my-service-name. Leave the default values for the other options.\n",
    "    3. Click Create.\n",
    "    \n",
    "    \n",
    "To get your service credentials:\n",
    "\n",
    "  * Copy your credentials from the *Service Details* page. To find the the *Service Details* page for an existing service, navigate to your Bluemix dashboard and click the service name.\n",
    "  * On the *Service Details* page, click *Service Credentials*, and then *View Credentials*.\n",
    "Copy username and password.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "echo \"IBM_WATSON_API_KEY='YOUR_API_KEY'\" >> ./api_keys.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import api_keys\n",
    "\n",
    "\n",
    "API_KEY = {\n",
    "  \"url\": \"https://gateway-a.watsonplatform.net/visual-recognition/api\",\n",
    "  \"note\": \"It may take up to 5 minutes for this key to become active\",\n",
    "  \"api_key\": api_keys.IBM_WATSON_API_KEY\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Network with Pre-trained Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See example:\n",
    "# https://github.com/watson-developer-cloud/python-sdk/blob/master/examples/visual_recognition_v3.py\n",
    "import json\n",
    "from watson_developer_cloud import VisualRecognitionV3\n",
    "\n",
    "\n",
    "test_url = 'https://www.ibm.com/ibm/ginni/images/ginni_bio_780x981_v4_03162016.jpg'\n",
    "\n",
    "visual_recognition = VisualRecognitionV3('2016-05-20', api_key=API_KEY['api_key'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "visual_recognition.classify?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets see what the Watson's Visual Recognition Classifier says to IBM's CEO Ginni Rometty.\n",
    "![](https://www.ibm.com/ibm/ginni/images/ginni_bio_780x981_v4_03162016.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"images\": [\n",
      "    {\n",
      "      \"classifiers\": [\n",
      "        {\n",
      "          \"classifier_id\": \"default\",\n",
      "          \"name\": \"default\",\n",
      "          \"classes\": [\n",
      "            {\n",
      "              \"class\": \"womans portrait photo\",\n",
      "              \"score\": 0.599,\n",
      "              \"type_hierarchy\": \"/person/female/woman/womans portrait photo\"\n",
      "            },\n",
      "            {\n",
      "              \"class\": \"woman\",\n",
      "              \"score\": 0.601\n",
      "            },\n",
      "            {\n",
      "              \"class\": \"female\",\n",
      "              \"score\": 0.602\n",
      "            },\n",
      "            {\n",
      "              \"class\": \"person\",\n",
      "              \"score\": 0.725\n",
      "            },\n",
      "            {\n",
      "              \"class\": \"adult person\",\n",
      "              \"score\": 0.559,\n",
      "              \"type_hierarchy\": \"/person/adult person\"\n",
      "            },\n",
      "            {\n",
      "              \"class\": \"people\",\n",
      "              \"score\": 0.5,\n",
      "              \"type_hierarchy\": \"/person/people\"\n",
      "            },\n",
      "            {\n",
      "              \"class\": \"light brown color\",\n",
      "              \"score\": 0.718\n",
      "            },\n",
      "            {\n",
      "              \"class\": \"beige color\",\n",
      "              \"score\": 0.533\n",
      "            }\n",
      "          ]\n",
      "        }\n",
      "      ],\n",
      "      \"source_url\": \"https://www.ibm.com/ibm/ginni/images/ginni_bio_780x981_v4_03162016.jpg\",\n",
      "      \"resolved_url\": \"https://www.ibm.com/ibm/ginni/images/ginni_bio_780x981_v4_03162016.jpg\"\n",
      "    }\n",
      "  ],\n",
      "  \"images_processed\": 1,\n",
      "  \"custom_classes\": 0\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(visual_recognition.classify(url=test_url), indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "visual_recognition.detect_faces?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"images\": [\n",
      "    {\n",
      "      \"faces\": [\n",
      "        {\n",
      "          \"age\": {\n",
      "            \"min\": 48,\n",
      "            \"max\": 51,\n",
      "            \"score\": 0.76694834\n",
      "          },\n",
      "          \"face_location\": {\n",
      "            \"height\": 245,\n",
      "            \"width\": 237,\n",
      "            \"left\": 286,\n",
      "            \"top\": 177\n",
      "          },\n",
      "          \"gender\": {\n",
      "            \"gender\": \"FEMALE\",\n",
      "            \"score\": 0.99999833\n",
      "          }\n",
      "        }\n",
      "      ],\n",
      "      \"source_url\": \"https://www.ibm.com/ibm/ginni/images/ginni_bio_780x981_v4_03162016.jpg\",\n",
      "      \"resolved_url\": \"https://www.ibm.com/ibm/ginni/images/ginni_bio_780x981_v4_03162016.jpg\"\n",
      "    }\n",
      "  ],\n",
      "  \"images_processed\": 1\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(visual_recognition.detect_faces(parameters=json.dumps({'url': test_url})), indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"classifiers\": []\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(visual_recognition.list_classifiers(), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To read more about what is possible via the Python API, see https://www.ibm.com/watson/developercloud/visual-recognition/api/v3/?python.\n",
    "\n",
    "In essence, the Python API is just wrapping HTTP REST API calls with the `requests` module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to classify a local image?\n",
    "\n",
    "There are many labeled datasets used in image recognition research. One of them is the Caltech 101 dataset, see http://www.vision.caltech.edu/Image_Datasets/Caltech101/. The actual dataset is here: http://www.vision.caltech.edu/Image_Datasets/Caltech101/101_ObjectCategories.tar.gz\n",
    "\n",
    "\n",
    "After downloading and and uncompressing the dataset, we can send the image of a butterfly to Watson.\n",
    "![](images/image_0027.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"images\": [\n",
      "    {\n",
      "      \"classifiers\": [\n",
      "        {\n",
      "          \"classifier_id\": \"default\",\n",
      "          \"name\": \"default\",\n",
      "          \"classes\": [\n",
      "            {\n",
      "              \"class\": \"monarch butterfly\",\n",
      "              \"score\": 0.87,\n",
      "              \"type_hierarchy\": \"/animal/invertebrate/insect/butterfly/monarch butterfly\"\n",
      "            },\n",
      "            {\n",
      "              \"class\": \"butterfly\",\n",
      "              \"score\": 0.977\n",
      "            },\n",
      "            {\n",
      "              \"class\": \"insect\",\n",
      "              \"score\": 0.977\n",
      "            },\n",
      "            {\n",
      "              \"class\": \"invertebrate\",\n",
      "              \"score\": 0.977\n",
      "            },\n",
      "            {\n",
      "              \"class\": \"animal\",\n",
      "              \"score\": 0.977\n",
      "            },\n",
      "            {\n",
      "              \"class\": \"viceroy butterfly\",\n",
      "              \"score\": 0.782,\n",
      "              \"type_hierarchy\": \"/animal/invertebrate/insect/butterfly/viceroy butterfly\"\n",
      "            },\n",
      "            {\n",
      "              \"class\": \"reddish orange color\",\n",
      "              \"score\": 0.889\n",
      "            },\n",
      "            {\n",
      "              \"class\": \"Indian red color\",\n",
      "              \"score\": 0.78\n",
      "            }\n",
      "          ]\n",
      "        }\n",
      "      ],\n",
      "      \"image\": \"./images/image_0027.jpg\"\n",
      "    }\n",
      "  ],\n",
      "  \"images_processed\": 1,\n",
      "  \"custom_classes\": 0\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "with open('./images/image_0027.jpg', 'rb') as image_file:\n",
    "    results = visual_recognition.classify(\n",
    "            images_file=image_file,\n",
    "            threshold='0.1')\n",
    "    print(json.dumps(results, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \n",
    "\n",
    "![](http://www.xpautographs.com/11337-7718-thickbox/mister-t-autograph.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"images\": [\n",
      "    {\n",
      "      \"classifiers\": [\n",
      "        {\n",
      "          \"classifier_id\": \"default\",\n",
      "          \"name\": \"default\",\n",
      "          \"classes\": [\n",
      "            {\n",
      "              \"class\": \"person\",\n",
      "              \"score\": 0.825\n",
      "            },\n",
      "            {\n",
      "              \"class\": \"people\",\n",
      "              \"score\": 0.5,\n",
      "              \"type_hierarchy\": \"/person/people\"\n",
      "            },\n",
      "            {\n",
      "              \"class\": \"Indian red color\",\n",
      "              \"score\": 0.97\n",
      "            }\n",
      "          ]\n",
      "        }\n",
      "      ],\n",
      "      \"source_url\": \"http://www.xpautographs.com/11337-7718-thickbox/mister-t-autograph.jpg\",\n",
      "      \"resolved_url\": \"http://www.xpautographs.com/11337-7718-thickbox/mister-t-autograph.jpg\"\n",
      "    }\n",
      "  ],\n",
      "  \"images_processed\": 1,\n",
      "  \"custom_classes\": 0\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "mister_t_url = 'http://www.xpautographs.com/11337-7718-thickbox/mister-t-autograph.jpg'\n",
    "print(json.dumps(visual_recognition.classify(url=mister_t_url), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://www.cphbusiness.dk/media/75910/lam.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"images\": [\n",
      "    {\n",
      "      \"classifiers\": [\n",
      "        {\n",
      "          \"classifier_id\": \"default\",\n",
      "          \"name\": \"default\",\n",
      "          \"classes\": [\n",
      "            {\n",
      "              \"class\": \"walrus mustache\",\n",
      "              \"score\": 0.559,\n",
      "              \"type_hierarchy\": \"/person/walrus mustache\"\n",
      "            },\n",
      "            {\n",
      "              \"class\": \"person\",\n",
      "              \"score\": 0.811\n",
      "            },\n",
      "            {\n",
      "              \"class\": \"beard\",\n",
      "              \"score\": 0.538,\n",
      "              \"type_hierarchy\": \"/person/beard\"\n",
      "            },\n",
      "            {\n",
      "              \"class\": \"mustachio\",\n",
      "              \"score\": 0.511,\n",
      "              \"type_hierarchy\": \"/person/mustachio\"\n",
      "            },\n",
      "            {\n",
      "              \"class\": \"person portrait photo\",\n",
      "              \"score\": 0.51,\n",
      "              \"type_hierarchy\": \"/person/person/person portrait photo\"\n",
      "            },\n",
      "            {\n",
      "              \"class\": \"taxonomist\",\n",
      "              \"score\": 0.504,\n",
      "              \"type_hierarchy\": \"/person/taxonomist\"\n",
      "            },\n",
      "            {\n",
      "              \"class\": \"adult person\",\n",
      "              \"score\": 0.5,\n",
      "              \"type_hierarchy\": \"/person/adult person\"\n",
      "            },\n",
      "            {\n",
      "              \"class\": \"reddish brown color\",\n",
      "              \"score\": 0.742\n",
      "            }\n",
      "          ]\n",
      "        }\n",
      "      ],\n",
      "      \"source_url\": \"https://www.cphbusiness.dk/media/75910/lam.png\",\n",
      "      \"resolved_url\": \"https://www.cphbusiness.dk/media/75910/lam.png\"\n",
      "    }\n",
      "  ],\n",
      "  \"images_processed\": 1,\n",
      "  \"custom_classes\": 0\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "lars_url = 'https://www.cphbusiness.dk/media/75910/lam.png'\n",
    "print(json.dumps(visual_recognition.classify(url=lars_url), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://www.cphbusiness.dk/media/74691/ltje.jpg) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"images\": [\n",
      "    {\n",
      "      \"faces\": [\n",
      "        {\n",
      "          \"age\": {\n",
      "            \"min\": 25,\n",
      "            \"max\": 28,\n",
      "            \"score\": 0.79470074\n",
      "          },\n",
      "          \"face_location\": {\n",
      "            \"height\": 949,\n",
      "            \"width\": 832,\n",
      "            \"left\": 333,\n",
      "            \"top\": 514\n",
      "          },\n",
      "          \"gender\": {\n",
      "            \"gender\": \"FEMALE\",\n",
      "            \"score\": 0.999997\n",
      "          }\n",
      "        }\n",
      "      ],\n",
      "      \"source_url\": \"https://www.cphbusiness.dk/media/74691/ltje.jpg\",\n",
      "      \"resolved_url\": \"https://www.cphbusiness.dk/media/74691/ltje.jpg\"\n",
      "    }\n",
      "  ],\n",
      "  \"images_processed\": 1\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "line_url = 'https://www.cphbusiness.dk/media/74691/ltje.jpg'\n",
    "print(json.dumps(visual_recognition.detect_faces(parameters=json.dumps({'url': line_url})), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detecting Many Faces in an Image\n",
    "\n",
    "![](http://www.albanyjobfair.com/wp-content/uploads/2014/01/BIz-people-2-300x276.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"images\": [\n",
      "    {\n",
      "      \"classifiers\": [\n",
      "        {\n",
      "          \"classifier_id\": \"default\",\n",
      "          \"name\": \"default\",\n",
      "          \"classes\": [\n",
      "            {\n",
      "              \"class\": \"wraparound\",\n",
      "              \"score\": 0.548,\n",
      "              \"type_hierarchy\": \"/garment/wraparound\"\n",
      "            },\n",
      "            {\n",
      "              \"class\": \"garment\",\n",
      "              \"score\": 0.548\n",
      "            },\n",
      "            {\n",
      "              \"class\": \"family\",\n",
      "              \"score\": 0.525,\n",
      "              \"type_hierarchy\": \"/person/family\"\n",
      "            },\n",
      "            {\n",
      "              \"class\": \"person\",\n",
      "              \"score\": 0.526\n",
      "            },\n",
      "            {\n",
      "              \"class\": \"people\",\n",
      "              \"score\": 0.563\n",
      "            },\n",
      "            {\n",
      "              \"class\": \"group of people\",\n",
      "              \"score\": 0.5\n",
      "            },\n",
      "            {\n",
      "              \"class\": \"alizarine red color\",\n",
      "              \"score\": 0.846\n",
      "            },\n",
      "            {\n",
      "              \"class\": \"ivory color\",\n",
      "              \"score\": 0.724\n",
      "            }\n",
      "          ]\n",
      "        }\n",
      "      ],\n",
      "      \"source_url\": \"http://www.albanyjobfair.com/wp-content/uploads/2014/01/BIz-people-2-300x276.jpg\",\n",
      "      \"resolved_url\": \"http://www.albanyjobfair.com/wp-content/uploads/2014/01/BIz-people-2-300x276.jpg\"\n",
      "    }\n",
      "  ],\n",
      "  \"images_processed\": 1,\n",
      "  \"custom_classes\": 0\n",
      "}\n",
      "{\n",
      "  \"images\": [\n",
      "    {\n",
      "      \"faces\": [\n",
      "        {\n",
      "          \"age\": {\n",
      "            \"min\": 32,\n",
      "            \"max\": 35,\n",
      "            \"score\": 0.8495456\n",
      "          },\n",
      "          \"face_location\": {\n",
      "            \"height\": 49,\n",
      "            \"width\": 45,\n",
      "            \"left\": 173,\n",
      "            \"top\": 91\n",
      "          },\n",
      "          \"gender\": {\n",
      "            \"gender\": \"MALE\",\n",
      "            \"score\": 0.9584368\n",
      "          }\n",
      "        },\n",
      "        {\n",
      "          \"age\": {\n",
      "            \"min\": 51,\n",
      "            \"max\": 55,\n",
      "            \"score\": 0.6513539\n",
      "          },\n",
      "          \"face_location\": {\n",
      "            \"height\": 52,\n",
      "            \"width\": 46,\n",
      "            \"left\": 239,\n",
      "            \"top\": 68\n",
      "          },\n",
      "          \"gender\": {\n",
      "            \"gender\": \"MALE\",\n",
      "            \"score\": 0.99932075\n",
      "          }\n",
      "        },\n",
      "        {\n",
      "          \"age\": {\n",
      "            \"min\": 23,\n",
      "            \"max\": 26,\n",
      "            \"score\": 0.7988043\n",
      "          },\n",
      "          \"face_location\": {\n",
      "            \"height\": 45,\n",
      "            \"width\": 36,\n",
      "            \"left\": 152,\n",
      "            \"top\": 50\n",
      "          },\n",
      "          \"gender\": {\n",
      "            \"gender\": \"MALE\",\n",
      "            \"score\": 0.9971029\n",
      "          }\n",
      "        },\n",
      "        {\n",
      "          \"age\": {\n",
      "            \"min\": 22,\n",
      "            \"max\": 26,\n",
      "            \"score\": 0.73620075\n",
      "          },\n",
      "          \"face_location\": {\n",
      "            \"height\": 42,\n",
      "            \"width\": 40,\n",
      "            \"left\": 31,\n",
      "            \"top\": 94\n",
      "          },\n",
      "          \"gender\": {\n",
      "            \"gender\": \"FEMALE\",\n",
      "            \"score\": 0.99993646\n",
      "          }\n",
      "        },\n",
      "        {\n",
      "          \"age\": {\n",
      "            \"min\": 19,\n",
      "            \"max\": 22,\n",
      "            \"score\": 0.99972486\n",
      "          },\n",
      "          \"face_location\": {\n",
      "            \"height\": 55,\n",
      "            \"width\": 41,\n",
      "            \"left\": 190,\n",
      "            \"top\": 136\n",
      "          },\n",
      "          \"gender\": {\n",
      "            \"gender\": \"FEMALE\",\n",
      "            \"score\": 0.999127\n",
      "          }\n",
      "        },\n",
      "        {\n",
      "          \"age\": {\n",
      "            \"min\": 20,\n",
      "            \"max\": 24,\n",
      "            \"score\": 0.6713774\n",
      "          },\n",
      "          \"face_location\": {\n",
      "            \"height\": 46,\n",
      "            \"width\": 38,\n",
      "            \"left\": 52,\n",
      "            \"top\": 144\n",
      "          },\n",
      "          \"gender\": {\n",
      "            \"gender\": \"FEMALE\",\n",
      "            \"score\": 0.9985127\n",
      "          }\n",
      "        },\n",
      "        {\n",
      "          \"age\": {\n",
      "            \"min\": 23,\n",
      "            \"max\": 26,\n",
      "            \"score\": 0.84344494\n",
      "          },\n",
      "          \"face_location\": {\n",
      "            \"height\": 49,\n",
      "            \"width\": 48,\n",
      "            \"left\": 228,\n",
      "            \"top\": 145\n",
      "          },\n",
      "          \"gender\": {\n",
      "            \"gender\": \"FEMALE\",\n",
      "            \"score\": 0.99955493\n",
      "          }\n",
      "        },\n",
      "        {\n",
      "          \"age\": {\n",
      "            \"min\": 28,\n",
      "            \"max\": 33,\n",
      "            \"score\": 0.51941246\n",
      "          },\n",
      "          \"face_location\": {\n",
      "            \"height\": 48,\n",
      "            \"width\": 33,\n",
      "            \"left\": 109,\n",
      "            \"top\": 134\n",
      "          },\n",
      "          \"gender\": {\n",
      "            \"gender\": \"FEMALE\",\n",
      "            \"score\": 0.99695385\n",
      "          }\n",
      "        },\n",
      "        {\n",
      "          \"age\": {\n",
      "            \"min\": 25,\n",
      "            \"max\": 28,\n",
      "            \"score\": 0.9299795\n",
      "          },\n",
      "          \"face_location\": {\n",
      "            \"height\": 46,\n",
      "            \"width\": 44,\n",
      "            \"left\": 45,\n",
      "            \"top\": 47\n",
      "          },\n",
      "          \"gender\": {\n",
      "            \"gender\": \"MALE\",\n",
      "            \"score\": 0.99030745\n",
      "          }\n",
      "        }\n",
      "      ],\n",
      "      \"source_url\": \"http://www.albanyjobfair.com/wp-content/uploads/2014/01/BIz-people-2-300x276.jpg\",\n",
      "      \"resolved_url\": \"http://www.albanyjobfair.com/wp-content/uploads/2014/01/BIz-people-2-300x276.jpg\"\n",
      "    }\n",
      "  ],\n",
      "  \"images_processed\": 1\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "group_url = 'http://www.albanyjobfair.com/wp-content/uploads/2014/01/BIz-people-2-300x276.jpg'\n",
    "\n",
    "print(json.dumps(visual_recognition.classify(url=group_url), indent=2))\n",
    "print(json.dumps(visual_recognition.detect_faces(parameters=json.dumps({'url': group_url})), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Training your own Classifier\n",
    "\n",
    "This is your task now. Take some images from different categories from the Caltech 101 dataset and train the neural network with them. For example, as in the following:\n",
    "\n",
    "```python\n",
    "with open('/path/to/butterflies.zip', 'rb') as butterflies, \\\n",
    "     open('/path/to/airplanes.zip'), 'rb') as airplanes:\n",
    "    print(json.dumps(visual_recognition.create_classifier('ButterfliesvsPlanes', \n",
    "                                                          butterflies_positive_examples=butterflies, \n",
    "                                                          negative_examples=airplanes), \n",
    "                     indent=2))\n",
    "```\n",
    "\n",
    "When you created some classifiers, you can list them as in the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"classifiers\": []\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(visual_recognition.list_classifiers(), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# How does this work? Introduction to Neural Networks.\n",
    "\n",
    "\n",
    "A next step from a single perceptron -as seen in the last lecture- to an image classifier as above is a Multi-layer Perceptron.\n",
    "\n",
    "![](http://www.saedsayad.com/images/Perceptron_bkp_1.png)\n",
    "\n",
    "\n",
    "\n",
    "The code in the following is adapted from Chapter 18 \"Neural Networks\" in the Data Science from Scratch book. The code can be found at: https://github.com/joelgrus/data-science-from-scratch/blob/master/code-python3/neural_networks.py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "random.seed(0)   # to get repeatable results\n",
    "input_size = 25  # each input is a vector of length 25\n",
    "\n",
    "num_hidden = 5   # we'll have 5 neurons in the hidden layer\n",
    "output_size = 10 # we need 10 outputs for each input\n",
    "\n",
    "# each hidden neuron has one weight per input, plus a bias weight\n",
    "hidden_layer = [[random.random() for _ in range(input_size + 1)]\n",
    "                 for _ in range(num_hidden)]\n",
    "\n",
    "# each output neuron has one weight per hidden neuron, plus a bias weight\n",
    "output_layer = [[random.random() for _ in range(num_hidden + 1)]\n",
    "                 for _ in range(output_size)]\n",
    "\n",
    "# the network starts out with random weights\n",
    "network = [hidden_layer, output_layer]\n",
    "\n",
    "\n",
    "print('Training...')\n",
    "\n",
    "# 10,000 iterations seems enough to converge\n",
    "for _ in tqdm(range(20000)):\n",
    "    for input_vector, target_vector in zip(inputs, targets):\n",
    "        backpropagate(network, input_vector, target_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## A Trainings Dataset\n",
    "\n",
    "We want to create a Multi-layer Perceptron, which can classifiy -or recognize- the digits from zero to nine for us. In `raw_digits` we create digits consisting out of 5x5 binary pixels. Consequently, each input in our trainings dataset is a binary vector of length 25."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "raw_digits = [\n",
    "        \"\"\"11111\n",
    "           1...1\n",
    "           1...1\n",
    "           1...1\n",
    "           11111\"\"\",\n",
    "        \"\"\"..1..\n",
    "           ..1..\n",
    "           ..1..\n",
    "           ..1..\n",
    "           ..1..\"\"\",\n",
    "        \"\"\"11111\n",
    "           ....1\n",
    "           11111\n",
    "           1....\n",
    "           11111\"\"\",\n",
    "        \"\"\"11111\n",
    "           ....1\n",
    "           11111\n",
    "           ....1\n",
    "           11111\"\"\",\n",
    "        \"\"\"1...1\n",
    "           1...1\n",
    "           11111\n",
    "           ....1\n",
    "           ....1\"\"\",\n",
    "        \"\"\"11111\n",
    "           1....\n",
    "           11111\n",
    "           ....1\n",
    "           11111\"\"\",\n",
    "        \"\"\"11111\n",
    "           1....\n",
    "           11111\n",
    "           1...1\n",
    "           11111\"\"\",\n",
    "        \"\"\"11111\n",
    "           ....1\n",
    "           ....1\n",
    "           ....1\n",
    "           ....1\"\"\",\n",
    "        \"\"\"11111\n",
    "           1...1\n",
    "           11111\n",
    "           1...1\n",
    "           11111\"\"\",\n",
    "        \"\"\"11111\n",
    "           1...1\n",
    "           11111\n",
    "           ....1\n",
    "           11111\"\"\"]\n",
    "\n",
    "\n",
    "def make_digit(raw_digit):\n",
    "    return [1 if c == '1' else 0\n",
    "            for row in raw_digit.split(\"\\n\")\n",
    "            for c in row.strip()]\n",
    "\n",
    "\n",
    "inputs = [make_digit(raw_digit) for raw_digit in raw_digits]\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "targets = [[1 if i == j else 0 for i in range(10)] for j in range(10)]\n",
    "targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For convience and reusabilty, we save the vectors containing the raw digits to a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "print(np.array(inputs, dtype=np.int8))\n",
    "#np.savetxt?\n",
    "np.savetxt('./simple_digit_trainingset.csv', np.array(inputs, dtype=np.int8), delimiter=',', fmt='%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "cat simple_digit_trainingset.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We create to helper functions, one for reading our trainings dataset from a file and a second one, which will plot it for inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "\n",
    "import csv\n",
    "import webget\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "filename = './simple_digit_trainingset.csv'\n",
    "\n",
    "\n",
    "def read_data(filename):\n",
    "    data = []\n",
    "    with open(filename) as f:\n",
    "        reader = csv.reader(f)\n",
    "        for row in reader:\n",
    "            label = reader.line_num - 1\n",
    "            image = np.array(row[:], dtype=np.int8)\n",
    "            data.append((label, image))\n",
    "    return data\n",
    "\n",
    "\n",
    "def generate_plot(data):\n",
    "    count = 0\n",
    "    f = plt.figure(figsize=(10, 5))\n",
    "    for idx, row in enumerate(data):\n",
    "        imarray = row[1].reshape((5, 5))\n",
    "        plt.subplot(2, 5, idx + 1)\n",
    "        plt.subplots_adjust(hspace=0.5)\n",
    "        count += 1\n",
    "        plt.title('Label = {}'.format(row[0]))\n",
    "        plt.imshow(imarray, cmap='Greys', interpolation='None')\n",
    "    return plt\n",
    "\n",
    "\n",
    "trainings_set = read_data(filename)\n",
    "plt.show(generate_plot(trainings_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## The Actual Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "def dot(v, w):\n",
    "    \"\"\"v_1 * w_1 + ... + v_n * w_n\"\"\"\n",
    "    return sum(v_i * w_i for v_i, w_i in zip(v, w))\n",
    "\n",
    "\n",
    "def sigmoid(t):\n",
    "    return 1 / (1 + np.exp(-t))\n",
    "\n",
    "\n",
    "def neuron_output(weights, inputs):\n",
    "    return sigmoid(dot(weights, inputs))\n",
    "\n",
    "\n",
    "def feed_forward(neural_network, input_vector):\n",
    "    \"\"\"takes in a neural network (represented as a list of lists of lists of weights)\n",
    "    and returns the output from forward-propagating the input\"\"\"\n",
    "\n",
    "    outputs = []\n",
    "\n",
    "    for layer in neural_network:\n",
    "\n",
    "        input_with_bias = input_vector + [1]             # add a bias input\n",
    "        output = [neuron_output(neuron, input_with_bias) # compute the output\n",
    "                  for neuron in layer]                   # for this layer\n",
    "        outputs.append(output)                           # and remember it\n",
    "\n",
    "        # the input to the next layer is the output of this one\n",
    "        input_vector = output\n",
    "\n",
    "    return outputs\n",
    "\n",
    "\n",
    "def predict(in_put):\n",
    "    return feed_forward(network, in_put)[-1]\n",
    "\n",
    "\n",
    "def backpropagate(network, input_vector, target):\n",
    "\n",
    "    hidden_outputs, outputs = feed_forward(network, input_vector)\n",
    "\n",
    "    # the output * (1 - output) is from the derivative of sigmoid\n",
    "    output_deltas = [output * (1 - output) * (output - target[i])\n",
    "                     for i, output in enumerate(outputs)]\n",
    "\n",
    "    # adjust weights for output layer (network[-1])\n",
    "    for i, output_neuron in enumerate(network[-1]):\n",
    "        for j, hidden_output in enumerate(hidden_outputs + [1]):\n",
    "            # print(i,j)\n",
    "            output_neuron[j] -= output_deltas[i] * hidden_output\n",
    "    #print('----')\n",
    "    # back-propagate errors to hidden layer\n",
    "    hidden_deltas = [hidden_output * (1 - hidden_output) *\n",
    "                      dot(output_deltas, [n[i] for n in network[-1]])\n",
    "                     for i, hidden_output in enumerate(hidden_outputs)]\n",
    "\n",
    "    # adjust weights for hidden layer (network[0])\n",
    "    for i, hidden_neuron in enumerate(network[0]):\n",
    "        for j, in_put in enumerate(input_vector + [1]):\n",
    "            hidden_neuron[j] -= hidden_deltas[i] * in_put"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## A Test-dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def plot_testset(data):\n",
    "    count = 0\n",
    "    f = plt.figure(figsize=(10, 5))\n",
    "    data = np.array(data)\n",
    "    for idx, row in enumerate(data):\n",
    "        imarray = row.reshape((5, 5))\n",
    "        plt.subplot(2, len(data), idx + 1)\n",
    "        plt.subplots_adjust(hspace=0.5)\n",
    "        count += 1\n",
    "        plt.imshow(imarray, cmap='Greys', interpolation='None')\n",
    "    return plt\n",
    "\n",
    "\n",
    "test_set = [[0,1,1,1,0,\n",
    "             0,0,0,1,1,\n",
    "             0,0,1,1,0,\n",
    "             0,0,0,1,1,\n",
    "             0,1,1,1,0],\n",
    "            [0,1,1,1,0,\n",
    "             1,0,0,1,1,\n",
    "             0,1,1,1,0,\n",
    "             1,0,0,1,1,\n",
    "             0,1,1,1,0],\n",
    "            [0,0,1,0,0,\n",
    "             0,0,1,0,0,\n",
    "             0,0,1,0,0,\n",
    "             0,0,1,0,0,\n",
    "             0,0,1,0,0],\n",
    "            [0,1,1,0,0,\n",
    "             0,0,1,0,0,\n",
    "             0,0,1,0,0,\n",
    "             0,0,1,0,0,\n",
    "             0,0,1,0,0]]\n",
    "print(test_set)\n",
    "plt.show(plot_testset(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "for test_data in test_set:\n",
    "    result = predict(test_data)\n",
    "    result = np.array(result)\n",
    "    print(np.argmax(result), np.array_str(result, precision=2, suppress_small=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "3 [0.   0.   0.   0.93 0.   0.   0.   0.01 0.   0.1 ]\n",
    "9 [0.   0.   0.   0.   0.   0.54 0.   0.   0.91 1.  ]\n",
    "1 [0.   0.96 0.03 0.02 0.   0.   0.   0.   0.   0.  ]\n",
    "3 [0.   0.22 0.   0.73 0.   0.   0.   0.   0.   0.  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "for test_data in inputs:\n",
    "    result = predict(test_data)\n",
    "    result = np.array(result)\n",
    "    print(np.argmax(result), np.array_str(result, precision=2, suppress_small=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Neural Networks Done Properly...\n",
    "\n",
    "And we still have a bit to go from classifying digits with a Multi-layer Perceptron to a Convolutional Neural Network (CNN), which is the technique that IBM applies in Watson for visual recognition. A modern framework for implementing various types of neural networks is Google's Tensorflow.\n",
    "\n",
    "\n",
    "```bash\n",
    "pip install tensorflow\n",
    "```\n",
    "\n",
    "You can get more information about it here:\n",
    "\n",
    "  * https://www.youtube.com/watch?v=qyvlt7kiQoI\n",
    "  * https://codelabs.developers.google.com/codelabs/cloud-tensorflow-mnist/#0\n",
    "  * https://github.com/martin-gorner/tensorflow-mnist-tutorial/blob/master/mnist_1.0_softmax.py\n",
    "  * https://en.wikipedia.org/wiki/MNIST_database\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Exercise!!!\n",
    "\n",
    "Your task is to extend the above example to work with the 'classical' MNIST dataset, which contains many thousands of handwritten digits. Your task is to watch the video https://hooktube.com/watch?v=wQ8BIBpya2k on HookTube (a lighter version of YouTube), which gives an introduction to Google's Tensorflow - a Python framework helping to build neural networks- and you follow the tutorial on https://codelabs.developers.google.com/codelabs/cloud-tensorflow-mnist.\n",
    "\n",
    "You have to reproduce their solution and the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Possible Projects:\n",
    "\n",
    "  * Implementation of a Salient Region Detector: http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.228.5552&rep=rep1&type=pdf\n",
    "  * Audio Fingerprinting: http://willdrevo.com/fingerprinting-and-audio-recognition-with-python/\n",
    "  * Survival on the Titanic: https://www.kaggle.com/c/titanic\n",
    "  * Predict forest cover: https://www.kaggle.com/c/forest-cover-type-prediction\n",
    "  * How to get free pizza: https://www.kaggle.com/c/random-acts-of-pizza"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
