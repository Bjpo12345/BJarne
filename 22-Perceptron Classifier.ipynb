{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Our dataset is again the iris flower dataset, see https://en.wikipedia.org/wiki/Iris_flower_data_set. Just that you know what these iris flowers are...\n",
    "\n",
    "\n",
    "**Iris Setosa**\n",
    "![iris_setosa](https://upload.wikimedia.org/wikipedia/commons/thumb/5/56/Kosaciec_szczecinkowaty_Iris_setosa.jpg/440px-Kosaciec_szczecinkowaty_Iris_setosa.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "\n",
    "**Iris Versicolor**\n",
    "![iris_versicolor](https://upload.wikimedia.org/wikipedia/commons/thumb/4/41/Iris_versicolor_3.jpg/440px-Iris_versicolor_3.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "\n",
    "**Iris Virginica**\n",
    "![iris_virginica](https://upload.wikimedia.org/wikipedia/commons/thumb/9/9f/Iris_virginica.jpg/440px-Iris_virginica.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![taxonomy](https://sebastianraschka.com/images/blog/2014/linear-discriminant-analysis/iris_petal_sepal.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "However, we simplify this dataset a bit and say we are only interested in *iris setosa* and other iris flowers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "colors = 'bgr'\n",
    "for idx, name in enumerate(iris.target_names):\n",
    "\n",
    "    sepal_length = iris.data[iris.target == idx][:,0]\n",
    "    sepal_width = iris.data[iris.target == idx][:,1]\n",
    "\n",
    "    plt.scatter(sepal_length, sepal_width, c=colors[idx], label=name)\n",
    "\n",
    "plt.title('Iris Sepal Characteristics')\n",
    "plt.xlabel('sepal length')\n",
    "plt.ylabel('sepal width')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "iris.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "sepal_length = iris.data[iris.target == 0][:,0]\n",
    "sepal_width = iris.data[iris.target == 0][:,1]\n",
    "plt.scatter(sepal_length, sepal_width, c='b', label='setosa')\n",
    "\n",
    "sepal_length = iris.data[iris.target != 0][:,0]\n",
    "sepal_width = iris.data[iris.target != 0][:,1]\n",
    "plt.scatter(sepal_length, sepal_width, c='g', label='other')\n",
    "\n",
    "plt.title('Iris Sepal Characteristics')\n",
    "plt.xlabel('sepal length')\n",
    "plt.ylabel('sepal width')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Intro to Perceptrons\n",
    "\n",
    "Let's say we want to write an artificial intelligence, i.e., a program, that learns out of the given data about sepal length and width, if a given iris flower is an iris setosa or if it is not.\n",
    "\n",
    "We can do this with a single artificial brain cell, a perceptron. The original paper for Perceptron Learning Algorithm (PLA) is from the 50s.\n",
    "\n",
    "*\"The Perceptron: A Probabilistic Model For Information Storage And Organization in the Brain\"* - F. Rosenblatt (December 1958)\n",
    "http://www.ling.upenn.edu/courses/Fall_2007/cogs501/Rosenblatt1958.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## A perceptron?\n",
    "\n",
    "A *perceptron* is a \n",
    "> hypothetical nervous system, or machine,...\n",
    "\n",
    "see the above paper. Or, according to the Oxford Dictionary, *perceptron* is a \n",
    "> a computer model or computerized machine devised to represent or simulate the ability of the brain to recognize and discriminate.\n",
    "\n",
    "\n",
    "![](https://appliedgo.net/media/perceptron/neuron.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "\n",
    "To see what is going on in a perceptron, see the animation at https://appliedgo.net/perceptron/ in the section *Inside an artificial neuron*.\n",
    "\n",
    "That is, we can implement an activation function and a perceptron in Python as in the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def activation_function(x):\n",
    "    if x < 0:\n",
    "        return -1\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "\n",
    "rnge = np.linspace(-10.0, 10.0, num=1000)\n",
    "values = [activation_function(i) for i in rnge]\n",
    "plt.plot(rnge, values)\n",
    "plt.axis([-10, 9, -2, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def perceptron(inp, weights):\n",
    "    # This is the same as the dot product np.dot(i, w)\n",
    "    dot_product = sum([i * w for i, w in zip(inp, weights)])\n",
    "    output = activation_function(dot_product)\n",
    "    return output\n",
    "\n",
    "perceptron([1, 2, 3, 4, 5], [1, 1, 2, 1, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Perceptrons for Classification\n",
    "\n",
    "Using our perceptron, we want to have a program which can *predict* -given a pair of sepal length and sepal width- if our flower is an iris setosa or not. That is, we want to have a program that says `setosa` (or `1`) for example for `[5.6, 4.8]` or for `[3.4, 4.1]` and it says `other` (or `-1`) for example for `[5.8, 1.9]` or for `[6.2, 2.4]`. Note that all those examples are \"new\" flowers, i.e., we did not observe those value combinations on earlier flowers. Additionally, our program should return the correct class labels for flowers, which we already know from the iris flower dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def predict(inp_vec, weights):\n",
    "    \"\"\"\n",
    "    inp_vec:\n",
    "        An input vector consisting of sepal length and width\n",
    "    return:\n",
    "        A class label, either 1 for 'setosa' or -1 for 'other'\n",
    "    \"\"\"\n",
    "    class_label = perceptron(inp_vec, weights)\n",
    "    return class_label\n",
    "\n",
    "\n",
    "weights = [-2.60105969, 3.61349319]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# new flowers\n",
    "assert predict([5.6, 4.8], weights) == 1\n",
    "assert predict([3.4, 4.1], weights) == 1\n",
    "assert predict([5.8, 1.9], weights) == -1\n",
    "assert predict([6.2, 2.4], weights) == -1\n",
    "\n",
    "# flowers we already know\n",
    "assert predict([5.4,  3.9], weights) == 1\n",
    "assert predict([5.7,  4.4], weights) == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Given the above program, we could say that we are done, no? The program obviously satisfies the requirements described above. However, the question is where do the weights (`weights = [-2.60105969, 3.61349319]`) come from? Here, I have given them to you but where did I get them from?\n",
    "\n",
    "Actually, they were automatically learned by the program using the Perceptron Learning Algorithm (PLA), which we are going to implement in the following.\n",
    "\n",
    "To do so, we need a trainings dataset. We will use the one in which we specified which iris flowers are of type setosa or of another type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class_labels = np.copy(iris.target)\n",
    "class_labels[class_labels != 0] = -1\n",
    "class_labels[class_labels == 0] = 1\n",
    "#class_labels[class_labels == -1] = 0\n",
    "\n",
    "trainings_data = [(d[:2], l) for d, l in zip(iris.data, class_labels)]\n",
    "trainings_data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from itertools import cycle\n",
    "\n",
    "\n",
    "def plot_data(data, w_line=(0, 0)):\n",
    "    \"\"\"\n",
    "    data:\n",
    "        \n",
    "    \"\"\"\n",
    "    # print(data[:10])\n",
    "    data_points, class_labels = list(zip(*data))\n",
    "    data_points = np.array(data_points)\n",
    "    class_labels = np.array(class_labels)\n",
    "\n",
    "    colors = cycle('bgrcmy')\n",
    "    for l, col in zip(np.unique(class_labels), colors):\n",
    "        d =  data_points[class_labels == l]\n",
    "        x = d[:,0]\n",
    "        y = d[:,1]\n",
    "        plt.scatter(d[:,0], d[:,1], c=col, label=l)\n",
    "    \n",
    "    if w_line != (0, 0):\n",
    "        l = np.linspace(0, 8.5)\n",
    "        m, b = w_line\n",
    "        plt.plot(l, m * l + b, 'y-', lw=2)\n",
    "\n",
    "    plt.axis([0, 8.5, 0, 5])\n",
    "    plt.title('Iris Characteristics')\n",
    "    plt.xlabel('length')\n",
    "    plt.ylabel('width')\n",
    "    plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plot_data(trainings_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def pla(training_data, no_iterations=10000, eta=0.5):\n",
    "    # eta is the learning rate\n",
    "    # initial_error\n",
    "    error = np.random.random()\n",
    "    dim = len(training_data[0][0])\n",
    "    weights =  np.random.random(dim)\n",
    "    weight_history = [np.copy(weights)]\n",
    "\n",
    "    for i in range(no_iterations):\n",
    "        inp_vec, expected_label = training_data[i % len(training_data)]\n",
    "        perceptron_output = perceptron(inp_vec, weights)\n",
    "        error = expected_label - perceptron_output\n",
    "        # print(error)\n",
    "        weights += eta * error * inp_vec\n",
    "        weight_history.append(np.copy(weights))\n",
    "        \n",
    "    return weights, weight_history \n",
    "        \n",
    "\n",
    "learned_weights, weight_history = pla(trainings_data)\n",
    "learned_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "weight_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# new flowers\n",
    "print(predict([5.6, 4.8], learned_weights))\n",
    "print(predict([3.4, 4.1], learned_weights))\n",
    "print(predict([5.8, 1.9], learned_weights))\n",
    "print(predict([6.2, 2.4], learned_weights))\n",
    "\n",
    "# flowers we already know\n",
    "print(predict([5.4, 3.9], learned_weights))\n",
    "print(predict([5.7, 4.4], learned_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# What was going on here?\n",
    "\n",
    "Let's visualize how the weights slice our feature space and how they developed over the iterations... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def compute_line(weights):\n",
    "    norm = np.linalg.norm(weights)\n",
    "    ww = weights / norm\n",
    "    ww1 = [ww[1], -ww[0]]\n",
    "    ww2 = [-ww[1] , ww[0]]\n",
    "                \n",
    "    # slope and intercept\n",
    "    m = (ww2[1] - ww1[1]) / (ww2[0] - ww1[0])\n",
    "    b = ((ww2[0] - ww1[0]) * m) + ww1[1]\n",
    "    \n",
    "    return m, b\n",
    "\n",
    "\n",
    "plot_data(trainings_data, compute_line(learned_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In the following you see how the weights and thereby the line discriminating our iris flower dataset. See the code that creates the animated Gif in the end of the notebook.\n",
    "\n",
    "![weights](./images/perceptron_anim.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# We add a bias input...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def pla(training_data, no_iterations=50000, w_bias=False, eta=0.5):\n",
    "    # eta is the learning rate\n",
    "    # initial_error\n",
    "    error = np.random.random()\n",
    "    dim = len(training_data[0][0])\n",
    "    weights = np.random.random(dim)\n",
    "    weight_history = [np.copy(weights)]\n",
    "\n",
    "    if w_bias:\n",
    "        weights = np.random.random(dim + 1)\n",
    "        data = np.array(list(zip(*training_data))[0])\n",
    "        biases = np.ones((data.shape[0], 1))\n",
    "        training_data_w_bias = np.append(data, biases, axis=1)\n",
    "        \n",
    "        training_data = list(zip(training_data_w_bias, \n",
    "                                 list(zip(*training_data))[1]))\n",
    "    \n",
    "    for i in range(no_iterations):\n",
    "        inp_vec, expected_label = training_data[i % len(training_data)]\n",
    "        perceptron_output = perceptron(inp_vec, weights)\n",
    "        error = expected_label - perceptron_output\n",
    "\n",
    "        weights += eta * error * inp_vec\n",
    "        weight_history.append(np.copy(weights))\n",
    "\n",
    "    return weights, np.array(weight_history)\n",
    "\n",
    "\n",
    "learned_weights, weight_history = pla(trainings_data, w_bias=True)\n",
    "learned_weights = learned_weights[:2]\n",
    "print(learned_weights)\n",
    "\n",
    "\n",
    "plot_data(trainings_data, compute_line(learned_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "print(predict([5.6, 4.8], learned_weights))\n",
    "print(predict([3.4, 4.1], learned_weights))\n",
    "print(predict([5.8, 1.9], learned_weights))\n",
    "print(predict([6.2, 2.4], learned_weights))\n",
    "\n",
    "# flowers we already know\n",
    "print(predict([5.4, 3.9], learned_weights))\n",
    "print(predict([5.7, 4.4], learned_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def compute_error_rate(data, model):\n",
    "    data_points, class_labels = list(zip(*data))\n",
    "    data_points = np.array(data_points)\n",
    "    class_labels = np.array(class_labels)\n",
    "    \n",
    "    errors = np.array([predict(vec, model) for vec in data_points])\n",
    "    c_falsy_elements = np.sum(errors != class_labels)\n",
    "    return c_falsy_elements / len(class_labels)\n",
    "\n",
    "\n",
    "error_rates = [compute_error_rate(trainings_data, i) for i in weight_history]\n",
    "\n",
    "plt.plot(error_rates)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def pla(training_data, no_iterations=10000, w_bias=False, eta=0.5):\n",
    "    # eta is the learning rate\n",
    "    # initial_error\n",
    "    error = np.random.random()\n",
    "    dim = len(training_data[0])\n",
    "    weights = np.random.random(dim)\n",
    "    weight_history = [np.copy(weights)]\n",
    "    error_rates = []\n",
    "\n",
    "    if w_bias:\n",
    "        weights = np.random.random(dim + 1)\n",
    "        data = np.array(list(zip(*training_data))[0])\n",
    "        biases = np.ones((data.shape[0], 1))\n",
    "        training_data_w_bias = np.append(data, biases, axis=1)\n",
    "        \n",
    "        training_data = list(zip(training_data_w_bias, \n",
    "                                 list(zip(*training_data))[1]))   \n",
    "    \n",
    "    if no_iterations == -1:\n",
    "        i = 0\n",
    "        error_rate = 1.0\n",
    "        while True:\n",
    "            inp_vec, expected_label = training_data[i % len(training_data)]\n",
    "            perceptron_output = perceptron(inp_vec, weights)\n",
    "            error = expected_label - perceptron_output\n",
    "            \n",
    "            weights += eta * error * inp_vec\n",
    "            weight_history.append(np.copy(weights))\n",
    "            \n",
    "            error_rate = compute_error_rate(training_data, weights)\n",
    "            error_rates.append(error_rate)\n",
    "            if error_rate < 0.01:\n",
    "                break\n",
    "            i += 1\n",
    "        print('Error {} after {} iiterations'.format(error_rate, i))\n",
    "    else:\n",
    "        for i in tqdm(range(no_iterations)):\n",
    "            inp_vec, expected_label = training_data[i % len(training_data)]\n",
    "            perceptron_output = perceptron(inp_vec, weights)\n",
    "            error = expected_label - perceptron_output\n",
    "            \n",
    "            weights += eta * error * inp_vec\n",
    "            weight_history.append(np.copy(weights))\n",
    "            \n",
    "            error_rate = compute_error_rate(training_data, weights)\n",
    "            error_rates.append(error_rate)\n",
    "        \n",
    "    return weights, np.array(weight_history), error_rates\n",
    "\n",
    "learned_weights, h, error_rates = pla(trainings_data, no_iterations=-1, w_bias=True)\n",
    "learned_weights = learned_weights[:2]\n",
    "print(learned_weights)\n",
    "\n",
    "\n",
    "plot_data(trainings_data, compute_line(learned_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# compute_error_rate(trainings_data, h[-1])\n",
    "\n",
    "# error_rates = [compute_error_rate(trainings_data, i) for i in h]\n",
    "\n",
    "plt.plot(error_rates)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Alternative Activation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.special import erf\n",
    "\n",
    "\n",
    "def activation_function(x):\n",
    "    if x < 0:\n",
    "        return -1\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "\n",
    "rnge = np.linspace(-10.0, 10.0, num=1000)\n",
    "values = [erf(i) for i in rnge]\n",
    "plt.plot(rnge, values)\n",
    "plt.axis([-10, 9, -2, 2])\n",
    "\n",
    "activation_function = erf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "learned_weights, weight_history, error_rates = pla(trainings_data, no_iterations=-1, w_bias=True)\n",
    "learned_weights = learned_weights[:2]\n",
    "print(learned_weights)\n",
    "\n",
    "\n",
    "plot_data(trainings_data, compute_line(learned_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "mkdir data_dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def plot_data(data, idx, w_line=(0, 0)):\n",
    "    plt.clf()\n",
    "    data_points, class_labels = list(zip(*data))\n",
    "    data_points = np.array(data_points)\n",
    "    class_labels = np.array(class_labels)\n",
    "\n",
    "    colors = cycle('bgrcmy')\n",
    "    for l, col in zip(np.unique(class_labels), colors):\n",
    "        d =  data_points[class_labels == l]\n",
    "        x = d[:,0]\n",
    "        y = d[:,1]\n",
    "        plt.scatter(d[:,0], d[:,1], c=col, label=l)\n",
    "    \n",
    "    if w_line != (0, 0):\n",
    "        l = np.linspace(0, 8.5)\n",
    "        m, b = w_line\n",
    "        plt.plot(l, m * l + b, 'y-', lw=2)\n",
    "\n",
    "    plt.axis([0, 8.5, 0, 5])\n",
    "    plt.title('Iris Characteristics {}'.format(idx))\n",
    "    plt.xlabel('length')\n",
    "    plt.ylabel('width')\n",
    "    plt.legend()\n",
    "    plt.savefig('./data_dump/p_it{}'.format(idx), dpi=100, bbox_inches='tight')\n",
    "\n",
    "    \n",
    "for idx, w in enumerate(weight_history[::80]):\n",
    "    plot_data(trainings_data, idx, compute_line(w[:2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "ls -ltrh ./data_dump/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# if you want to have this piece of code working on your VM run\n",
    "# sudo apt-get install -y imagemagick\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "basedir = './data_dump/'\n",
    "pngs = [pl for pl in os.listdir(basedir) if pl.endswith('png')]\n",
    "sorted_pngs = sorted(pngs, key=lambda a:int(a.split('p_it')[1][:-4]), reverse=False)\n",
    "pngs = [os.path.join(basedir, png) for png in sorted_pngs]\n",
    "pngs_str = ' '.join(pngs)\n",
    "cmd = (\"convert -delay 10 {} ./data_dump/animated.gif\".format(pngs_str)).split()\n",
    "proc = subprocess.Popen(cmd, stdin = subprocess.PIPE, stdout = subprocess.PIPE)\n",
    "out, err = proc.communicate()\n",
    "\n",
    "if not err:\n",
    "    for png in pngs:\n",
    "        os.remove(png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Learning Observed\n",
    "\n",
    "![weights](./data_dump/animated.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Exercise\n",
    "\n",
    "  * Implement the Perceptron Learning Algorithm (PLA) similarly as demonstrated above for the petal length and petal width.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "petal_length = transposed_data[2]\n",
    "petal_width = transposed_data[3]\n",
    "\n",
    "\n",
    "# The following lines are only to generate a list of labels, which may \n",
    "# only be numbers\n",
    "labels = list(set(species))\n",
    "number_labels = [labels.index(s) for s in species]\n",
    "colors = ['red', 'green', 'blue']\n",
    "\n",
    "plt.scatter(petal_length, petal_width, marker=\"o\", \n",
    "            c=number_labels, cmap=matplotlib.colors.ListedColormap(colors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "  * Adapt the PLA algorithm to work on another dataset. For example, create a dataset with checkmarks and crosses that may appear on paper forms and train your perceptron to discriminate those."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "  * Implement the Perceptron Learning Algorithm (PLA) similarly as demonstrated above for the following three-dimensional dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "\n",
    "\n",
    "centers = [[2, 1, 0], [0, -1, -1]]\n",
    "data_3d, _ = make_blobs(n_samples=2500, centers=centers, cluster_std=0.37)\n",
    "x, y, z = data_3d[:,0], data_3d[:,1], data_3d[:,2]\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(x, y, z, linewidth=0.2)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# What else can I do with a single Perceptron?\n",
    "\n",
    "For example, winning Flappy Bird. It will likely become quickly better than you!\n",
    "\n",
    "But note, this neural network is not trained as described above -with gradient descent-. Instead it uses a genetic algorithms to find new weights for neuronal networks of future generations. That is, use some of the best individuals. Some randomly generated ones and some created by breeding from existing individuals.\n",
    "\n",
    "https://github.com/xviniette/FlappyLearning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import IFrame\n",
    "IFrame('http://xviniette.github.io/FlappyLearning/', width=550, height=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Cleaned up version for teaching!\n",
    "# PLA -- Perceptron Learning Algorithm\n",
    "# https://blog.dbrgn.ch/2013/3/26/perceptrons-in-python/\n",
    "# from numpy import array, dot, random\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "class Perceptron():\n",
    "\n",
    "    def __init__(self, training_data, no_iterations=10000):\n",
    "        # we fill our initial weights wit random numbers\n",
    "        # vector w looks for example like: [0.9, 0.08]\n",
    "        self.w = np.array([random.random() for i in range(2)])\n",
    "        self.errors = []\n",
    "        # eta is the learning rate...\n",
    "        self.eta = 0.05\n",
    "        self.no_iterations = no_iterations\n",
    "        self.iter_no = 0\n",
    "        self._lines = []\n",
    "        # zip together the training data, it will have the form:\n",
    "        # [((meassured_length, meassured_width), label), ...]\n",
    "        self.training_data = training_data\n",
    "        \n",
    "        # self._plt = self._plot_data()\n",
    "                \n",
    "    def unit_step(self, x):\n",
    "        if x < 0:\n",
    "            return -1\n",
    "        else:\n",
    "            return 1\n",
    "\n",
    "    def pla(self):\n",
    "        iteration = 0\n",
    "        error = random.random()\n",
    "        # for x, expected in training_data * 100:\n",
    "        for i in range(self.no_iterations):\n",
    "        # while error != 0 or iteration > self.no_iterations:\n",
    "        # while iteration > self.no_iterations:\n",
    "            # idx = random.choice(range(len(training_data) - 1))\n",
    "            x, expected = training_data[i % len(training_data)]\n",
    "            x = np.array(x)\n",
    "            result = np.dot(self.w, x)\n",
    "\n",
    "            error = expected - self.unit_step(result)\n",
    "            self.errors.append(error)\n",
    "            self.w += self.eta * error * x\n",
    "            self.iter_no = i\n",
    "            self.plot_all(save_img=True, from_training=True)\n",
    "            \n",
    "    def predict(self, meassured_length_width_pair):\n",
    "        x = np.array(meassured_length_width_pair)\n",
    "        result = np.dot(self.w, meassured_length_width_pair)\n",
    "        found_class = self.unit_step(result)\n",
    "        return found_class\n",
    "\n",
    "    def _plot_line(self):\n",
    "        # Eukledian norm\n",
    "        # norm = math.sqrt(self.w[0] ** 2 + self.w[1] ** 2)\n",
    "        norm = np.linalg.norm(self.w)\n",
    "        \n",
    "        ww = self.w / norm\n",
    "        \n",
    "        ww1 = [ww[1], -ww[0]]\n",
    "        ww2 = [-ww[1] , ww[0]]\n",
    "        \n",
    "        l = np.linspace(-10,10)\n",
    "        \n",
    "        # slope and intercept\n",
    "        m = (ww2[1] - ww1[1]) / (ww2[0] - ww1[0])\n",
    "        b = ((ww2[0] - ww1[0]) * m) + ww1[1]\n",
    "        \n",
    "        plt.plot(l, m * l + b, 'y-', lw=3)\n",
    "        \n",
    "        return plt\n",
    "    \n",
    "    def _plot_data(self, colors=['red', 'green']):\n",
    "        plt.xlabel('sepal length')\n",
    "        plt.ylabel('sepal width')\n",
    "\n",
    "        data, label = list(zip(*self.training_data))\n",
    "        width, length = list(zip(*data))\n",
    "        \n",
    "        plt.scatter(width, length , marker=\"o\", c=label, \n",
    "                    cmap=matplotlib.colors.ListedColormap(colors))\n",
    "        plt.axis([4, 8.5, 1.5, 5])\n",
    "        \n",
    "        return plt\n",
    "    \n",
    "    def _assemble_plot(self, save_img=False):\n",
    "        if self.iter_no != 0:\n",
    "            msg = 'Iris Characteristics (Iteration {})'.format(\n",
    "                self.iter_no)\n",
    "        else:\n",
    "            msg = 'Iris Characteristics'\n",
    "        # plt.figure(self.iter_no + 1)\n",
    "        plt.clf()\n",
    "        plt.title(msg)\n",
    "        self._plot_data()\n",
    "        self._plot_line()\n",
    "        \n",
    "        if save_img:\n",
    "            self._save_plot()\n",
    "            \n",
    "    def plot_all(self, save_img=False, from_training=False):\n",
    "        \n",
    "        if not from_training:\n",
    "            self._assemble_plot(save_img=save_img)\n",
    "            plt.figure(self.iter_no)\n",
    "        else:\n",
    "            if self.iter_no % 100 == 0:\n",
    "                self._assemble_plot(save_img=save_img)\n",
    "\n",
    "        return plt\n",
    "    \n",
    "    def _save_plot(self):            \n",
    "        plt.savefig('./data_dump/p_it{}'.format(self.iter_no), \n",
    "                     dpi=200, bbox_inches='tight')\n",
    "\n",
    "\n",
    "\n",
    "training_data = list(zip(zip(sepal_length, sepal_width), \n",
    "                         bin_number_labels))\n",
    "        \n",
    "p = Perceptron(training_data, no_iterations=6000)\n",
    "p.pla()\n",
    "print(p.w)\n",
    "p.plot_all()\n",
    "# [-2.34181064  3.39946769]"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
