{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Getting a Video Stream\n",
    "\n",
    "\n",
    "You can read a vidoe frame by frame from a file or from a camera connected to your computer. To do so, you have to instantiate `cv2.VideoCapture`. In case you give it a path to a movie file it reads frames from a file. Otherwise, if you pass an integer it grabs frames from the camera with the corresponding images.\n",
    "\n",
    "https://docs.opencv.org/master/d0/da7/videoio_overview.html\n",
    "\n",
    "https://docs.opencv.org/master/d8/dfe/classcv_1_1VideoCapture.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Grab frames from a file and scale the frames down.\n",
    "\n",
    "```python\n",
    "import cv2\n",
    "\n",
    "\n",
    "cap = cv2.VideoCapture('/path/to/a/movie/file')  \n",
    "\n",
    "w, h = (640, 400)\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, w)\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, h)\n",
    "```\n",
    "\n",
    "Grab frames from the first camera connected to your computer.\n",
    "\n",
    "```python\n",
    "import cv2\n",
    "\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "```    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "'''\n",
    "Retrofying\n",
    "\n",
    "USAGE:\n",
    "    retrofyme.py\n",
    "'''\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import scipy.spatial as sp\n",
    "\n",
    "\n",
    "def _c64_colors():\n",
    "    # From https://upload.wikimedia.org/wikipedia/commons/6/65/Commodore64_palette.png\n",
    "    # with Seashore, the 16 C64 colors\n",
    "    black = [0, 0, 0]\n",
    "    white = [255, 255, 255]\n",
    "    red = [154, 76, 67]\n",
    "    cyan = [122, 194, 200]\n",
    "    purple = [157, 90, 165]\n",
    "    green = [103, 171, 95]\n",
    "    blue = [82, 73, 156]\n",
    "    yellow = [202, 212, 137]\n",
    "    a = [156, 103, 58]\n",
    "    b = [106, 82, 12]\n",
    "    c = [197, 126, 119]\n",
    "    d = [99, 99, 99]\n",
    "    e = [139, 139, 139]\n",
    "    f = [164, 226, 157]\n",
    "    g = [139, 130, 205]\n",
    "    hc = [175, 175, 175]\n",
    "\n",
    "    c64_colors = [black, white, red, cyan, purple, green, blue, yellow, a, b,\n",
    "                  c, d, e, f, g, hc]\n",
    "    return np.array(c64_colors)\n",
    "\n",
    "\n",
    "def create_tree(colors):\n",
    "    return sp.cKDTree(colors)\n",
    "\n",
    "\n",
    "def query_tree(small_image, tree):\n",
    "    h, w, d = small_image.shape\n",
    "\n",
    "    small_image_lst = small_image.reshape(h * w, d)\n",
    "    # get Euclidean distance and index of each C64 color in tree\n",
    "    _, result = tree.query(small_image_lst)\n",
    "\n",
    "    for idx, c in enumerate(_c64_colors()):\n",
    "        small_image_lst[result == idx] = c\n",
    "    return small_image_lst\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    w, h = (320, 200)\n",
    "    cap.set(cv2.CAP_PROP_FRAME_WIDTH, w)\n",
    "    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, h)\n",
    "\n",
    "    tree = create_tree(_c64_colors())\n",
    "\n",
    "    while True:\n",
    "        ret, img = cap.read()\n",
    "        _ = query_tree(img, tree)\n",
    "\n",
    "        img_large = cv2.resize(img, (1280, 800),\n",
    "                               interpolation=cv2.INTER_NEAREST)\n",
    "        cv2.imshow('Retrofied', img_large)\n",
    "\n",
    "        if cv2.waitKey(1) == 27:\n",
    "            break\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Filtering/Smoothing an Image\n",
    "\n",
    "\n",
    "The following examples and descriptions are adapted from\n",
    "http://docs.opencv.org/3.1.0/d4/d13/tutorial_py_filtering.html\n",
    "\n",
    "Image blurring is achieved by convolving the image with a low-pass filter kernel. It is useful for removing noises. It actually removes high frequency content (eg: noise, edges) from the image. So edges are blurred a little bit in this operation. (Well, there are blurring techniques which does not blur the edges too). OpenCV provides mainly four types of blurring techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import webget\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "url = 'http://pngnq.sourceforge.net/testimages/mandrill.png'\n",
    "webget.download(url)\n",
    "\n",
    "img = cv2.imread(os.path.basename(url))\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "blur = cv2.GaussianBlur(img, (5, 5), 0)\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.axis('off')\n",
    "plt.title('Original')\n",
    "plt.imshow(img, interpolation='none')  # , cmap=plt.cm.Greys_r)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title('Modified')\n",
    "plt.imshow(blur, interpolation='none')  #, cmap=plt.cm.Greys_r)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "kernel_x = cv2.getGaussianKernel(5, 0)\n",
    "print(kernel_x * kernel_x.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Thresholds and Binary Images\n",
    "\n",
    "The following examples and descriptions are adapted from\n",
    "http://docs.opencv.org/3.1.0/d7/d4d/tutorial_py_thresholding.html\n",
    "\n",
    "If pixel value is greater than a threshold value, it is assigned one value (may be white), else it is assigned another value (may be black). The function used is `cv2.threshold`. First argument is the source image, which should be a grayscale image. Second argument is the threshold value which is used to classify the pixel values. Third argument is the `maxVal` which represents the value to be given if pixel value is more than the threshold value. OpenCV provides different styles of thresholding and it is decided by the fourth parameter of the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import webget\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "img = cv2.imread('./mare-08.jpg')\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "print(img)\n",
    "img = cv2.medianBlur(img, 5)\n",
    "\n",
    "ret, thresh = cv2.threshold(img, 127, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title('Original')\n",
    "plt.imshow(img, interpolation='none', cmap=plt.cm.Greys_r)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title('Modified')\n",
    "plt.imshow(thresh, interpolation='none', cmap=plt.cm.Greys_r)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Morphological Operations\n",
    "\n",
    "The following examples and descriptions are adapted from\n",
    "https://docs.opencv.org/4.1.0/d9/d61/tutorial_py_morphological_ops.html\n",
    "\n",
    "Morphological transformations are some simple operations based on the image shape. It is normally performed on binary images. It needs two inputs, one is our original image, second one is called structuring element or kernel which decides the nature of operation. Two basic morphological operators are Erosion and Dilation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Erosion\n",
    "\n",
    "The basic idea of erosion is just like soil erosion only, it erodes away the boundaries of foreground object. The kernel slides through the image (as in 2D convolution). A pixel in the original image (either 1 or 0) will be considered 1 only if all the pixels under the kernel is 1, otherwise it is eroded (made to zero).\n",
    "\n",
    "Here, all the pixels near the boundary will be discarded depending upon the size of kernel. So the thickness or size of the foreground object decreases or simply white region decreases in the image. It is useful for removing small white noises (as we have seen in colorspace chapter), detach two connected objects etc.\n",
    "\n",
    "Here is an example of a 5x5 kernel with full of ones and a 3x3 kernel with *cross*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import webget\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "webget.download('http://docs.opencv.org/3.1.0/j.png')\n",
    "\n",
    "img = cv2.imread('j.png', 0)\n",
    "kernel = np.ones((5, 5), np.uint8)\n",
    "kernel2 = np.array([[0,1,0],\n",
    "                    [1,1,1],\n",
    "                    [0,1,0]], np.uint8)\n",
    "\n",
    "\n",
    "erosion = cv2.erode(img, kernel2, iterations=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plt.subplot(1, 2, 1)\n",
    "plt.title('Original')\n",
    "plt.imshow(img, interpolation='none', cmap=plt.cm.Greys_r)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title('Modified')\n",
    "plt.imshow(erosion, interpolation='none', cmap=plt.cm.Greys_r)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Dilation\n",
    "\n",
    "It is just opposite of erosion. Here, a pixel element is '1' if atleast one pixel under the kernel is '1'. So it increases the white region in the image or size of foreground object increases. Normally, in cases like noise removal, erosion is followed by dilation. Because, erosion removes white noises, but it also shrinks our object. So we dilate it. Since noise is gone, they won't come back, but our object area increases. It is also useful in joining broken parts of an object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "img = cv2.imread('j.png', 0)\n",
    "kernel = np.ones((5, 5), np.uint8)\n",
    "kernel2 = np.array([[0,1,0],\n",
    "                    [1,1,1],\n",
    "                    [0,1,0]], np.uint8)\n",
    "\n",
    "dilation = cv2.dilate(img, kernel, iterations=1)\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title('Original')\n",
    "plt.imshow(img, interpolation='none', cmap=plt.cm.Greys_r)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title('Modified')\n",
    "plt.imshow(dilation, interpolation='none', cmap=plt.cm.Greys_r)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "###  Opening\n",
    "\n",
    "Opening is just another name of *erosion* followed by *dilation*. It is useful in removing noise, as we explained above. Here we use the function, `cv2.morphologyEx()`.\n",
    "\n",
    "Let's start by generating a bit of noise to distort our image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "noise = np.random.randint(2, size=112*150)\n",
    "noise = noise.reshape(150, 112).astype(np.uint8)\n",
    "plt.imshow(noise, cmap=plt.cm.Greys, interpolation='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "img = cv2.imread('j.png', 0)\n",
    "kernel = np.ones((5, 5), np.uint8)\n",
    "kernel2 = np.array([[0,1,0],\n",
    "                    [1,1,1],\n",
    "                    [0,1,0]], np.uint8)\n",
    "img = img - noise\n",
    "opening = cv2.morphologyEx(img, cv2.MORPH_OPEN, kernel)\n",
    "\n",
    "erosion = cv2.erode(img, kernel, iterations=1)\n",
    "opening_stepwise = cv2.dilate(erosion, kernel, iterations=1)\n",
    "assert np.array_equal(opening, opening_stepwise)\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title('Original')\n",
    "plt.imshow(img, interpolation='none', cmap=plt.cm.Greys_r)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title('Modified')\n",
    "plt.imshow(opening, interpolation='none', cmap=plt.cm.Greys_r)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Closing\n",
    "\n",
    "Closing is reverse of Opening, Dilation followed by Erosion. It is useful in closing small holes inside the foreground objects, or small black points on the object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "img = cv2.imread('j.png', 0)\n",
    "kernel = np.ones((5, 5), np.uint8)\n",
    "kernel2 = np.array([[0,1,0],\n",
    "                    [1,1,1],\n",
    "                    [0,1,0]], np.uint8)\n",
    "img = img + noise\n",
    "closing = cv2.morphologyEx(img, cv2.MORPH_CLOSE, kernel)\n",
    "\n",
    "dilation = cv2.dilate(img, kernel, iterations=1)\n",
    "closing_stepwise = cv2.erode(dilation, kernel, iterations=1)\n",
    "assert np.array_equal(closing, closing_stepwise)\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title('Original')\n",
    "plt.imshow(img, interpolation='none', cmap=plt.cm.Greys_r)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title('Modified')\n",
    "plt.imshow(closing_stepwise, interpolation='none', cmap=plt.cm.Greys_r)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# An Example\n",
    "\n",
    "There is a citizen science project *Chimp&See* (https://www.chimpandsee.org/) from the Max Planck Institute for Evolutionary Anthropology, which asks for help on identifying different animals moving through the djungle.\n",
    "https://www.chimpandsee.org/\n",
    "\n",
    "\n",
    "Let's see how we can try to help them with a naive implementation of movement detection and tracking, \n",
    "see program `movement_detection.py`, e.g., `python movement_detection.py --movie=570d07dd3fc33c0001bee21b.mp4 --area=400 --amp=3`.\n",
    "\n",
    "This program is inspired by and adapted from \n",
    "http://www.pyimagesearch.com/2015/05/25/basic-motion-detection-and-tracking-with-python-and-opencv/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](./movement.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](./movement_fd.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](./movement_thresh.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "python movement_detection.py --movie=test.mp4 --area=400 --amp=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "python movement_detection.py --movie=/dev/video0 --area=400 --amp=3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Exercise\n",
    "\n",
    "* Study `movement_detection.py`\n",
    "  - What is happening in the line?\n",
    "  - Is there something you do not understand?\n",
    "* Find a small video clip and run it through the `movement_detection.py`\n",
    "  - Make sure there is some form of moment in the video (preferably of people)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "\"\"\"Detect movement and track moving objects in a video stream.\n",
    "\n",
    "Usage:\n",
    "  movement_detection.py [--movie=<path>] [--area=<size>] [--amp=<amount>]\n",
    "  movement_detection.py (-h | --help)\n",
    "  movement_detection.py --version\n",
    "\n",
    "Options:\n",
    "  -h --help      Show this screen.\n",
    "  --version      Show version.\n",
    "  --area=<size>  Area of varying pixels [default: 50].\n",
    "  --amp=<amount> Amplification factor of diff image [default: 1].\n",
    "  --movie=<path> Path to movie file.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# python movement_detection.py --movie=/Volumes/DATA\\ SHARE/570d03ea3fc33c0001be9e41.mp4 --area=10\n",
    "# python movement_detection.py --movie=/Volumes/DATA\\ SHARE/570d07dd3fc33c0001bee21b.mp4 --area=60\n",
    "# python movement_detection.py --movie=/Volumes/DATA\\ SHARE/570d08b63fc33c0001bef03c.mp4 --area=10\n",
    "\n",
    "# python movement_detection.py --movie=/Volumes/DATA\\ SHARE/570d08b63fc33c0001bef03c.mp4 --area=500\n",
    "\n",
    "\n",
    "__version__ = 0.1\n",
    "\n",
    "\n",
    "import cv2\n",
    "import time\n",
    "import numpy as np\n",
    "from docopt import docopt\n",
    "\n",
    "\n",
    "\n",
    "first_frame = None\n",
    "text = 'Nothing to see...'\n",
    "\n",
    "\n",
    "def get_video_input(movie_file):\n",
    "    if movie_file is None:\n",
    "        cap = cv2.VideoCapture(0)\n",
    "        time.sleep(0.25)\n",
    " \n",
    "    # otherwise, we are reading from a video file\n",
    "    else:\n",
    "        cap = cv2.VideoCapture(movie_file)  \n",
    "\n",
    "    w, h = (640, 400)\n",
    "    cap.set(cv2.CAP_PROP_FRAME_WIDTH, w)\n",
    "    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, h)\n",
    "\n",
    "    return cap\n",
    "\n",
    "\n",
    "def process_frame(frame, amp):\n",
    "    global first_frame\n",
    "\n",
    "    # convert the frame to grayscale, and blur it\n",
    "   \n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    gray = cv2.GaussianBlur(gray, (21, 21), 0)\n",
    "\n",
    "    # if the first frame is None, initialize it\n",
    "    if first_frame is None:\n",
    "        first_frame = gray\n",
    "\n",
    "    # compute the absolute difference between the current frame and\n",
    "    # first frame\n",
    "    frame_delta = cv2.absdiff(first_frame, gray) * amp\n",
    "    thresh = cv2.threshold(frame_delta, 40, 255, cv2.THRESH_BINARY)[1]\n",
    " \n",
    "    # dilate the thresholded image to fill in holes, then find contours\n",
    "    # on thresholded image\n",
    "    thresh = cv2.dilate(thresh, None, iterations=2)\n",
    "\n",
    "    return thresh, frame_delta\n",
    "\n",
    "\n",
    "def find_contours(frame, thresh, area):\n",
    "    global text\n",
    "    cnts = cv2.findContours(thresh.copy(), cv2.RETR_EXTERNAL, \n",
    "                            cv2.CHAIN_APPROX_SIMPLE)[-2]\n",
    "    # loop over the contours\n",
    "    for c in cnts:\n",
    "        # if the contour is too small, ignore it\n",
    "        if cv2.contourArea(c) >= area:\n",
    " \n",
    "            # compute the bounding box for the contour, draw it on the frame,\n",
    "            # and update the text\n",
    "            (x, y, w, h) = cv2.boundingRect(c)\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "            text = 'Something is moving!'\n",
    "\n",
    "\n",
    "def run(movie_file, area=50, amp=1, save=False):\n",
    "    global text\n",
    "\n",
    "    is_fst_frame = True\n",
    "    cap = get_video_input(movie_file)\n",
    "    frames = []\n",
    "    \n",
    "    # main loop\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if is_fst_frame:\n",
    "            is_fst_frame = False\n",
    "            continue\n",
    "        # print(frame.shape)\n",
    "        # if the frame could not be grabbed, then we have reached the end\n",
    "        # of the video\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        text = 'Nothing to see...'\n",
    " \n",
    "        thresh, frame_delta = process_frame(frame, amp)\n",
    "        find_contours(frame, thresh, area)\n",
    "\n",
    "        # draw the text on the frame\n",
    "        cv2.putText(frame, \"Status: {}\".format(text), (10, 20),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)\n",
    "        \n",
    "        # show the frame and record if the user presses a key\n",
    "        cv2.imshow(\"Video Stream\", frame)\n",
    "        cv2.imshow(\"Threshold\", thresh)\n",
    "        cv2.imshow(\"Frame Delta\", frame_delta)\n",
    "\n",
    "        frames.append((frame, thresh, frame_delta))\n",
    "        if cv2.waitKey(5) == 27:\n",
    "            break\n",
    "\n",
    "    if save: \n",
    "        for idx, f_triplet in enumerate(frames):\n",
    "            f, t, fd = f_triplet\n",
    "            if idx % 4 == 0:\n",
    "                cv2.imwrite('./data/frame_{num:03d}.png'.format(num=idx), f)\n",
    "                cv2.imwrite('./data/thresh_{num:03d}.png'.format(num=idx), t)\n",
    "                cv2.imwrite('./data/framed_{num:03d}.png'.format(num=idx), fd)\n",
    "        # create the animated gif with:\n",
    "        # convert -loop 0 -scale 75% $(ls ./data/frame_*.png | sort) ./data/movement.gif\n",
    "\n",
    "    # cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    arguments = docopt(__doc__, version=__version__)\n",
    "    print(arguments)\n",
    "    #time.sleep(2)\n",
    "    if arguments['--amp'] is None:\n",
    "        amp = 1\n",
    "    else:\n",
    "        amp = int(arguments['--amp'])\n",
    "    run(arguments['--movie'], area=int(arguments['--area']), \n",
    "        amp=amp)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
